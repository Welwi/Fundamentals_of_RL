{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RL_typo.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOqqyZTODQRNc/ZX8H3J+76",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Welwi/RL_typo/blob/master/RL_typo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEqdvGemP8Bs",
        "colab_type": "text"
      },
      "source": [
        "In this project, I am training a RL algorithm that will master two varying envs with varying complexity.\n",
        "\n",
        "ENV 1: Cartpole\n",
        "The goal is to balance a pole, portruding from a cart, in an upright position by only movign the base left or right. This is an env with a low-dimensional observation space.\n",
        "\n",
        "ENV2: Pong\n",
        "The goal is to beat the competition. The env has a high-dimensional observation space - learning directly from raw pixels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCezM37-SGV1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "outputId": "c734d39e-d12f-4894-f414-4750055e620a"
      },
      "source": [
        "\n",
        "!apt-get install -y xvfb python-opengl x11-utils > /dev/null 2>&1\n",
        "!pip install gym pyvirtualdisplay scikit-video > /dev/null 2>&1\n",
        "\n",
        "!pip install mitdeeplearning"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting mitdeeplearning\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8b/3b/b9174b68dc10832356d02a2d83a64b43a24f1762c172754407d22fc8f960/mitdeeplearning-0.1.2.tar.gz (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 4.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from mitdeeplearning) (1.18.5)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from mitdeeplearning) (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from mitdeeplearning) (4.41.1)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (from mitdeeplearning) (0.17.2)\n",
            "Requirement already satisfied: cloudpickle<1.4.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym->mitdeeplearning) (1.3.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym->mitdeeplearning) (1.5.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym->mitdeeplearning) (1.4.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym->mitdeeplearning) (0.16.0)\n",
            "Building wheels for collected packages: mitdeeplearning\n",
            "  Building wheel for mitdeeplearning (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mitdeeplearning: filename=mitdeeplearning-0.1.2-cp36-none-any.whl size=2114586 sha256=48db7795b5cf778f217b4f026667ae7f9a343053121a85a2c242658669a21a0a\n",
            "  Stored in directory: /root/.cache/pip/wheels/27/e1/73/5f01c787621d8a3c857f59876c79e304b9b64db9ff5bd61b74\n",
            "Successfully built mitdeeplearning\n",
            "Installing collected packages: mitdeeplearning\n",
            "Successfully installed mitdeeplearning-0.1.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-QQGQA2TIL-",
        "colab_type": "text"
      },
      "source": [
        "## Steps of RL probs in general:\n",
        "1. Initialize the env and the agent: describe the different observations and actions the agent can make in the env.\n",
        "\n",
        "2. Define the agent's memory: this will enable the agent to remember its past actions, observations and rewards\n",
        "\n",
        "3. Define a reward function: describes the reward associated with an action or sequence of actions\n",
        "\n",
        "4. Define the learning algorithm: this is used to reinforce the agent's good behavior and discourage the bad behaviors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2GrwZ3YP7tj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "import base64, io, time, gym\n",
        "import IPython, functools\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "import mitdeeplearning as mdl"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-eOAk5PPbrG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}