{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RL_typo.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO7GzkS3oxb/B2MG3YnnZo0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Welwi/RL_typo/blob/master/RL_typo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEqdvGemP8Bs",
        "colab_type": "text"
      },
      "source": [
        "In this project, I am training a RL algorithm that will master two varying envs with varying complexity.\n",
        "\n",
        "ENV 1: Cartpole\n",
        "The goal is to balance a pole, portruding from a cart, in an upright position by only movign the base left or right. This is an env with a low-dimensional observation space.\n",
        "\n",
        "ENV2: Pong\n",
        "The goal is to beat the competition. The env has a high-dimensional observation space - learning directly from raw pixels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCezM37-SGV1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "outputId": "79f65247-e5cf-4b33-acfc-69d66f6ff809"
      },
      "source": [
        "\n",
        "!apt-get install -y xvfb python-opengl x11-utils > /dev/null 2>&1\n",
        "!pip install gym pyvirtualdisplay scikit-video > /dev/null 2>&1\n",
        "\n",
        "!pip install mitdeeplearning"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting mitdeeplearning\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8b/3b/b9174b68dc10832356d02a2d83a64b43a24f1762c172754407d22fc8f960/mitdeeplearning-0.1.2.tar.gz (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from mitdeeplearning) (1.18.5)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from mitdeeplearning) (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from mitdeeplearning) (4.41.1)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (from mitdeeplearning) (0.17.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym->mitdeeplearning) (1.4.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym->mitdeeplearning) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.4.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym->mitdeeplearning) (1.3.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym->mitdeeplearning) (0.16.0)\n",
            "Building wheels for collected packages: mitdeeplearning\n",
            "  Building wheel for mitdeeplearning (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mitdeeplearning: filename=mitdeeplearning-0.1.2-cp36-none-any.whl size=2114586 sha256=e50c07e7648636502796948e84070ccc05f8050fa1c44bcadfa797676ae1ec67\n",
            "  Stored in directory: /root/.cache/pip/wheels/27/e1/73/5f01c787621d8a3c857f59876c79e304b9b64db9ff5bd61b74\n",
            "Successfully built mitdeeplearning\n",
            "Installing collected packages: mitdeeplearning\n",
            "Successfully installed mitdeeplearning-0.1.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-QQGQA2TIL-",
        "colab_type": "text"
      },
      "source": [
        "## Steps of RL probs in general:\n",
        "1. Initialize the env and the agent: describe the different observations and actions the agent can make in the env.\n",
        "\n",
        "2. Define the agent's memory: this will enable the agent to remember its past actions, observations and rewards\n",
        "\n",
        "3. Define a reward function: describes the reward associated with an action or sequence of actions\n",
        "\n",
        "4. Define the learning algorithm: this is used to reinforce the agent's good behavior and discourage the bad behaviors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2GrwZ3YP7tj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "import base64, io, time, gym\n",
        "import IPython, functools\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "import mitdeeplearning as mdl"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B99XtJvdUH0T",
        "colab_type": "text"
      },
      "source": [
        "## PART 1: CARTPOLE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIrE3x5JU2hY",
        "colab_type": "text"
      },
      "source": [
        "Gym is a toolkit that has several pre-defined environments for training and testing RL learning agents.\n",
        "\n",
        "In Cartpole, the pole starts upright and the goal is to prevent it from falling. A reward of +1 is given for every timestep that the pole remains upright.\n",
        "A reward of -1 is given if the the pole is more than 15 degrees from the vertical or if the cart moves more than 2.4 units from the center of the track."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-eOAk5PPbrG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f057e600-7883-4507-dec1-70bc291ce1b8"
      },
      "source": [
        "# Instantiating the cartpole env\n",
        "\n",
        "env = gym.make('CartPole-v0')\n",
        "env.seed(1)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-qrXGmeYCyk",
        "colab_type": "text"
      },
      "source": [
        "Observations that help define the env:\n",
        "1. cart position\n",
        "2. cart velocity\n",
        "3. pole angle\n",
        "4. pole rotation rate\n",
        "\n",
        "Actions that the agent can take:\n",
        "- The agent can move either right or left.\n",
        "\n",
        "This shows that this is a low-dimensional observation and action spaces."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zo7NiaNhV_xq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "3266a41b-314e-4537-ed61-7e96740243e1"
      },
      "source": [
        "# Checking the size of the space\n",
        "n_observations = env.observation_space\n",
        "print('Env has observation space =', n_observations)\n",
        "\n",
        "# Checking the num of actions that the agent can take\n",
        "n_actions = env.action_space.n\n",
        "print('Num of possible actions that the agent can choose from =', n_actions)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Env has observation space = Box(4,)\n",
            "Num of possible actions that the agent can choose from = 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_C4MHuqffUOM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "834dcc13-2dfa-4477-99b6-a13a4e8ae1cd"
      },
      "source": [
        "n_actions"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k27CIunnayPu",
        "colab_type": "text"
      },
      "source": [
        "### Defining the agent\n",
        "\n",
        "In RL, a deep neural network defines the agent.\n",
        "This network takes in an observation of the environment, and outputs the probability of taking each of the possible actions.\n",
        "Since this is a low dimensional observation space, we can use a simple feed forward NN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IsK2HjcNZEUi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defining the carpole agent\n",
        "def create_carpole_model():\n",
        "\n",
        "  model = tf.keras.models.Sequential([\n",
        "                                      tf.keras.layers.Dense(units=32, activation='relu'),\n",
        "                                      tf.keras.layers.Dense(units=n_actions, activation= None)\n",
        "  ])\n",
        "\n",
        "  return model\n",
        "\n",
        "cartpole_model = create_carpole_model()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1qk2pYMgR41",
        "colab_type": "text"
      },
      "source": [
        "Defining a feed forward pass through the network (action function)\n",
        "\n",
        "- takes observations as inputs\n",
        "- does a forward pass through the model\n",
        "- outputs the agent action\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMmUy-vZfe8u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def choose_action(model, observation):\n",
        "\n",
        "  # adding the batch dimension to the observation\n",
        "  observation = np.expand_dims(observation, axis=0)\n",
        "\n",
        "  # passing the observation through the model\n",
        "  logits = model.predict(observation)\n",
        "\n",
        "  # pass the probabilities through softmax to get true probability\n",
        "  prob_weights = tf.nn.softmax(logits).numpy()\n",
        "\n",
        "  # random selection of an action of an action from observation\n",
        "  action = np.random.choice(n_actions, size=1, p=prob_weights.flatten())[0]\n",
        "\n",
        "  return action"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGYuoOfR8MfQ",
        "colab_type": "text"
      },
      "source": [
        "## Agents Memory\n",
        "\n",
        "Unables agent to remember past actions, observations and rewards.\n",
        "\n",
        "In RL:\n",
        "- Training happens alongside the agent's acting in the env\n",
        "- Episode: sequence of actions that ends in a terminal state (pole falling or cart crashing)\n",
        "- The agent needs to remember all of the observations and actions for the episode to reinforce or punish the actions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6w6E6sdovPz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lhCYRSdxov0e",
        "colab_type": "text"
      },
      "source": [
        "## Breaking it down"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Pj2RXPzlKSk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# resets the state of the env and returns an initial observation\n",
        "observation = env.reset()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sc7iHkkql9ch",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4f9f870a-faf7-428e-921c-8be636d9f398"
      },
      "source": [
        "observation"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.03073904,  0.00145001, -0.03088818, -0.03131252])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYF3VDmUnGEu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "05576618-e95d-4b57-fd3d-07892332c727"
      },
      "source": [
        "observation.shape"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5pi3WyTymcMU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# adding batch dimension to the obs\n",
        "observation = np.expand_dims(observation, axis=0)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UDDOtwIm-9L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "41e8e5b1-ed2a-4a13-c6de-f3cb1d28e287"
      },
      "source": [
        "observation.shape"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 4)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOPkbTp0l-O2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "664db347-1407-4a34-c553-6b445d445767"
      },
      "source": [
        "# passing obs through the model\n",
        "logits = cartpole_model(observation)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
            "\n",
            "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
            "\n",
            "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6mWzrG7mO4r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "496db20f-3db4-4e12-bcfc-cd859a9c4240"
      },
      "source": [
        "logits"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[-0.00845183,  0.02072986]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cG2rpdkPmn3d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  # pass the probabilities through softmax to get true probability\n",
        "  prob_weights = tf.nn.softmax(logits).numpy()"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5EDFnpO-muKh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ad98f7c4-4695-486d-fe34-bf3290a15c72"
      },
      "source": [
        "prob_weights"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.49270508, 0.5072949 ]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oh09DrRXoRQx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d4e42126-001a-4466-e2a0-543010e54cb6"
      },
      "source": [
        "prob_weights.flatten()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.49270508, 0.5072949 ], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1-jqCOHoY1_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ed6ca6e4-91ac-4ada-da4e-c14966241d74"
      },
      "source": [
        "prob_weights.shape"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqFSOYgHobzl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6ae967ed-989f-42e4-8bf1-6422a970b521"
      },
      "source": [
        "prob_weights.flatten().shape"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAnibykcmvxl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "action = np.random.choice(n_actions, size=1, p=prob_weights.flatten())"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_fEob4aYolUd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0e5ed57a-f222-4c4f-c00b-69f205163293"
      },
      "source": [
        "action"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGMsrE_komPB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "958dde36-2a20-48a3-a2f7-d81692bc9255"
      },
      "source": [
        "action[0]"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYDgstsAonog",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "action = np.random.choice(n_actions, size=1, p=prob_weights.flatten())[0]"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9KApPaPI1tkD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 23,
      "outputs": []
    }
  ]
}